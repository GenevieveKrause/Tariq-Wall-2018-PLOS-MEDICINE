{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "metadata = []\n",
    "\n",
    "with open('primary_dataset.csv', 'r') as f:\n",
    "    data = list(csv.reader(f))\n",
    "\n",
    "header = data[0][6:36]\n",
    "for line in data[1:]:\n",
    "    scorer, video, diagnosis, age, sex = line[2], line[3], line[36], line[37], line[38]\n",
    "    qs = line[6:36]\n",
    "        \n",
    "    # Remove scores with missing questions\n",
    "    if len([x for x in qs if x == '']) == 0:\n",
    "        X.append([int(x) for x in qs])\n",
    "        y.append(int(diagnosis == 'asd'))\n",
    "        metadata.append([scorer, video, age, sex])\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "m, n = X.shape\n",
    "\n",
    "print('X', X.shape)\n",
    "print('y', y.shape)\n",
    "print('videos', len(set([x[1] for x in metadata])))\n",
    "print('scorers', len(set([x[0] for x in metadata])))\n",
    "print('% missing', np.sum(X>=7)/(X.shape[0]*X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode responses\n",
    "X_one_hot = []\n",
    "header_one_hot = []\n",
    "q_to_start_index = []\n",
    "for q in range(30):\n",
    "    q_to_start_index.append(len(header_one_hot))\n",
    "    for response in np.unique(X[:, q]):\n",
    "        if response < 7:\n",
    "            X_one_hot.append(list(X[:, q]==response))\n",
    "            header_one_hot.append((header[q], response))\n",
    "q_to_start_index.append(len(header_one_hot))\n",
    "X_one_hot = np.asarray(X_one_hot).T\n",
    "print('X one hot', X_one_hot.shape)\n",
    "print('X one hot header', len(header_one_hot))\n",
    "print('q_to_start_index', q_to_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent test data\n",
    "Xind = []\n",
    "yind = []\n",
    "metadataind = []\n",
    "\n",
    "with open('microbiome_updated.csv', 'r') as f:\n",
    "    data = list(csv.reader(f))\n",
    "\n",
    "for line in data[1:]:\n",
    "    scorer, video, diagnosis, age, sex = line[2], line[3], line[36], line[37], line[38]\n",
    "    qs = line[6:36]\n",
    "        \n",
    "    # Remove scores with missing questions\n",
    "    if len([x for x in qs if x == '']) == 0:\n",
    "        Xind.append([int(x) for x in qs])\n",
    "        yind.append(int(diagnosis == 'asd'))\n",
    "        metadataind.append([scorer, video, age, sex])\n",
    "\n",
    "Xind = np.asarray(Xind)\n",
    "yind = np.asarray(yind)\n",
    "print(Xind.shape, yind.shape)\n",
    "print('scorers', len(set([x[0] for x in metadataind])))\n",
    "print('videos', len(set([x[1] for x in metadataind])))\n",
    "print('% missing', np.sum(Xind>=7)/(Xind.shape[0]*Xind.shape[1]))\n",
    "\n",
    "# one hot encode responses\n",
    "Xind_one_hot = []\n",
    "for q in range(30):\n",
    "    for response in np.unique(X[:, q]):\n",
    "        if response < 7:\n",
    "            Xind_one_hot.append(list(Xind[:, q]==response))\n",
    "Xind_one_hot = np.asarray(Xind_one_hot).T\n",
    "print(Xind_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median impute missing values in raw data\n",
    "meds = np.asarray([np.median(z[z<7]) for z in X.T])\n",
    "missing_indices = np.where(X>=7)\n",
    "print(X[missing_indices])\n",
    "X[missing_indices] = meds[missing_indices[1]]\n",
    "print(X[missing_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test\n",
    "We tried splitting by video and also by rater. At first, we thought correlation might exists between two different raters rating the same video. However, after some experimentation we noticed that each rater seems to have his/her own technique and we were overfitting to rater more than to video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "def split_by_video(videos, k):\n",
    "    shuffled_videos = list(videos)\n",
    "    random.shuffle(shuffled_videos)\n",
    "    num_videos_per_group = int(math.floor(len(shuffled_videos)/k))\n",
    "    split = []\n",
    "    videos = []\n",
    "    for i in range(k):\n",
    "        split_videos = set(shuffled_videos[(num_videos_per_group*i):(num_videos_per_group*(i+1))])\n",
    "        split.append(list(np.where([x[1] in split_videos for x in metadata])[0]))\n",
    "        videos.append(split_videos)\n",
    "    return split, videos\n",
    "\n",
    "def split_by_rater(raters, k):\n",
    "    shuffled_raters = list(raters)\n",
    "    random.shuffle(shuffled_raters)\n",
    "    num_raters_per_group = int(math.floor(len(shuffled_raters)/k))\n",
    "    split = []\n",
    "    raters = []\n",
    "    for i in range(k):\n",
    "        split_raters = set(shuffled_raters[(num_raters_per_group*i):(num_raters_per_group*(i+1))])\n",
    "        split.append(list(np.where([x[0] in split_raters for x in metadata])[0]))\n",
    "        raters.append(split_raters)\n",
    "    return split, raters\n",
    "\n",
    "\n",
    "unique_videos = set([x[1] for x in metadata])\n",
    "print('unique videos', len(unique_videos))\n",
    "\n",
    "unique_raters = set([x[0] for x in metadata])\n",
    "print('unique raters', len(unique_raters))\n",
    "\n",
    "split, videos = split_by_video(unique_videos, 5)\n",
    "test_indices, test_videos = np.asarray(split[0]), videos[0]\n",
    "train_indices, train_videos = np.asarray(list(chain.from_iterable(split[1:5]))), set(chain.from_iterable(videos[1:5]))\n",
    "\n",
    "#split, raters = split_by_rater(unique_raters, 3)\n",
    "#test_indices, test_raters = np.asarray(split[0]), raters[0]\n",
    "#train_indices, train_raters = np.asarray(list(chain.from_iterable(split[1:3]))), set(chain.from_iterable(raters[1:3]))\n",
    "\n",
    "print('train_videos', len(train_videos))\n",
    "print('test_videos', len(test_videos))\n",
    "#print('train raters', len(train_raters))\n",
    "#print('test raters', len(test_raters))\n",
    "print(train_indices.shape, test_indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic L1 and L2 Models\n",
    "Take a quick look at performance of L2 vs L1 regularization. However, we need to take these results with a grain of salt since I haven't tuned any of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def print_stats(model, X_train, y_train, X_test, y_test):\n",
    "    # Nonzero entries\n",
    "    print('nonzero coefficients:', np.sum(model.coef_[0]!=0), 'out of', model.coef_.shape[1])\n",
    "\n",
    "    # Accuracy\n",
    "    print('train accuracy:', sgdc.score(X_train, y_train))\n",
    "    print('test accuracy:', sgdc.score(X_test, y_test))\n",
    "    \n",
    "    # AUC\n",
    "    yhat = sgdc.predict_proba(X_train)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, yhat)\n",
    "    print('train auc:', auc(fpr, tpr))\n",
    "    yhat = sgdc.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, yhat)\n",
    "    print('test auc:', auc(fpr, tpr))\n",
    "\n",
    "# ---------------------------- L2 ----------------------------\n",
    "print('L2 Raw Data')\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l2', alpha=0.1, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n",
    "\n",
    "print('\\nL2 One-hot Data')\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l2', alpha=0.1, fit_intercept=True)\n",
    "sgdc.fit(X_one_hot[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X_one_hot[train_indices, :], y[train_indices], X_one_hot[test_indices, :], y[test_indices])\n",
    "\n",
    "# ---------------------------- L1 ----------------------------\n",
    "print('\\nL1 Raw Data')\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l1', alpha=0.05, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n",
    "\n",
    "print('\\nL1 One-hot Data')\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l1', alpha=0.05, fit_intercept=True)\n",
    "sgdc.fit(X_one_hot[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X_one_hot[train_indices, :], y[train_indices], X_one_hot[test_indices, :], y[test_indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we notice that as expected, that L2 outperforms L1 (it gets to use many more features to make predictions). However, even with a small number of features, and no parameter tuning, the L1 models are doing pretty well. This is promising. Comparing raw data to one-hot encoded data, it looks like the one-hot encoding may be performing better, but the numbers are pretty close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune an L2 Model\n",
    "We start by tuning an L2 model. This let's us know the best possible performance we can expect from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "def tune_L2(X, k, alphas):\n",
    "    accuracy_avg = []\n",
    "    accuracy_err = []\n",
    "    auc_avg = []\n",
    "    auc_err = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        accuracy = np.zeros((k,))\n",
    "        auroc = np.zeros((k,))\n",
    "\n",
    "        for i, (split_test_indices, split_test_videos) in enumerate(zip(*split_by_video(train_videos, k))):\n",
    "            split_train_indices = list(set(train_indices) - set(split_test_indices))\n",
    "\n",
    "            # fit model\n",
    "            sgdc = SGDClassifier(loss=\"log\", penalty='l2', alpha=alpha, fit_intercept=True)\n",
    "            sgdc.fit(X[split_train_indices, :], y[split_train_indices])\n",
    "\n",
    "            # accuracy\n",
    "            accuracy[i] = sgdc.score(X[split_test_indices, :], y[split_test_indices])\n",
    "\n",
    "            # auc\n",
    "            yhat = sgdc.predict_proba(X[split_test_indices, :])[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y[split_test_indices], yhat)\n",
    "            auroc[i] = auc(fpr, tpr)\n",
    "\n",
    "        # calculate mean and 95% confidence intervals\n",
    "        accuracy_avg.append(np.mean(accuracy))\n",
    "        ci = st.t.interval(0.95, k-1, loc=np.mean(accuracy), scale=st.sem(accuracy))\n",
    "        accuracy_err.append((ci[1]-ci[0])/2)\n",
    "        auc_avg.append(np.mean(auroc))\n",
    "        ci = st.t.interval(0.95, k-1, loc=np.mean(auroc), scale=st.sem(auroc))\n",
    "        auc_err.append((ci[1]-ci[0])/2)\n",
    "    return accuracy_avg, accuracy_err, auc_avg, auc_err\n",
    "    \n",
    "\n",
    "k = 5\n",
    "alphas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "accuracy_avg_raw, accuracy_err_raw, auc_avg_raw, auc_err_raw = tune_L2(X, k, alphas)\n",
    "accuracy_avg_onehot, accuracy_err_onehot, auc_avg_onehot, auc_err_onehot = tune_L2(X_one_hot, k, alphas)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Accuracy\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(alphas, accuracy_avg_raw, label='raw data')\n",
    "plt.fill_between(alphas, [x+e for x, e in zip(accuracy_avg_raw, accuracy_err_raw)], [x-e for x, e in zip(accuracy_avg_raw, accuracy_err_raw)], alpha=0.5)\n",
    "\n",
    "plt.plot(alphas, accuracy_avg_onehot, label='one-hot data')\n",
    "plt.fill_between(alphas, [x+e for x, e in zip(accuracy_avg_onehot, accuracy_err_onehot)], [x-e for x, e in zip(accuracy_avg_onehot, accuracy_err_onehot)], alpha=0.5)\n",
    "\n",
    "plt.xlabel('Regularization Parameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.6, 1])\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "# AUC\n",
    "plt.subplot(2, 1, 2, sharex=ax1)\n",
    "\n",
    "plt.plot(alphas, auc_avg_raw, label='raw data')\n",
    "plt.fill_between(alphas, [x+e for x, e in zip(auc_avg_raw, auc_err_raw)], [x-e for x, e in zip(auc_avg_raw, auc_err_raw)], alpha=0.5)\n",
    "\n",
    "plt.plot(alphas, auc_avg_onehot, label='one-hot data')\n",
    "plt.fill_between(alphas, [x+e for x, e in zip(auc_avg_onehot, auc_err_onehot)], [x-e for x, e in zip(auc_avg_onehot, auc_err_onehot)], alpha=0.5)\n",
    "\n",
    "plt.xlabel('Regularization Parameter')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim([0.6, 1])\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us a couple of things. First, the raw data and the one-hot encoded data seem to perform approximately the same. However, it's important to keep in mind that L2 models use all of the features, so this may change when we introduce L1 regularization and the number of features decreases. Second, our error bars are fairly wide. Third, the best AUC we can hope for is about 0.95, and the best accuracy is about 0.85. Looking at the left side of the plots, very small amounts of regularization give you fairly good accuracy but low AUC. Looking at the right side of the plots, large amounts of regularization improve AUC, but decrease accuracy. The plots also give us a range of regularization parameters (alpha) to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune an Elastic Net Model\n",
    "The challenge in tuning an elastic net model is that we have two parameters to consider:\n",
    "    1. alpha: the amount of regularization \n",
    "    2. l1_ratio: the ratio between L1 and L2 regularization\n",
    "We're interested in understanding how performance (measured by accuracy and AUC) vary with the number of parameters used. To do this, we'll start by fixing alpha, and then varying the l1_ratio to see a range of models with differing number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_l1_ratio(X, k, alpha, l1_ratios):\n",
    "    accuracy_avg = []\n",
    "    accuracy_err = []\n",
    "    auc_avg = []\n",
    "    auc_err = []\n",
    "    nzcoeffs_median = []\n",
    "\n",
    "    for l1_ratio in l1_ratios:\n",
    "        accuracy = np.zeros((k,))\n",
    "        auroc = np.zeros((k,))\n",
    "        nzcoeffs = np.zeros((k,))\n",
    "\n",
    "        for i, (split_test_indices, split_test_videos) in enumerate(zip(*split_by_video(train_videos, k))):\n",
    "            split_train_indices = list(set(train_indices) - set(split_test_indices))\n",
    "\n",
    "            # fit model\n",
    "            sgdc = SGDClassifier(loss=\"log\", penalty='elasticnet', alpha=alpha, l1_ratio=l1_ratio, fit_intercept=True)\n",
    "            sgdc.fit(X[split_train_indices, :], y[split_train_indices])\n",
    "\n",
    "            # accuracy\n",
    "            accuracy[i] = sgdc.score(X[split_test_indices, :], y[split_test_indices])\n",
    "\n",
    "            # auc\n",
    "            yhat = sgdc.predict_proba(X[split_test_indices, :])[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y[split_test_indices], yhat)\n",
    "            auroc[i] = auc(fpr, tpr)\n",
    "            \n",
    "            # nonzero coeffs\n",
    "            nzcoeffs[i] = np.sum(sgdc.coef_[0]!=0)\n",
    "\n",
    "        # calculate mean and 95% confidence intervals\n",
    "        accuracy_avg.append(np.mean(accuracy))\n",
    "        ci = st.t.interval(0.95, k-1, loc=np.mean(accuracy), scale=st.sem(accuracy))\n",
    "        accuracy_err.append((ci[1]-ci[0])/2)\n",
    "        auc_avg.append(np.mean(auroc))\n",
    "        ci = st.t.interval(0.95, k-1, loc=np.mean(auroc), scale=st.sem(auroc))\n",
    "        auc_err.append((ci[1]-ci[0])/2)\n",
    "        nzcoeffs_median.append(np.median(nzcoeffs))\n",
    "    return accuracy_avg, accuracy_err, auc_avg, auc_err, nzcoeffs_median\n",
    "\n",
    "k = 5\n",
    "alphas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "l1_ratios = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex='col', sharey=True, figsize=(10, 10))\n",
    "axes[0, 0].axhline(y=0.85, linestyle='--', color='black', alpha=0.5)\n",
    "axes[0, 1].axhline(y=0.85, linestyle='--', color='black', alpha=0.5)\n",
    "axes[1, 0].axhline(y=0.95, linestyle='--', color='black', alpha=0.5)\n",
    "axes[1, 1].axhline(y=0.95, linestyle='--', color='black', alpha=0.5)\n",
    "\n",
    "for alpha in alphas:\n",
    "    accuracy_avg_raw, accuracy_err_raw, auc_avg_raw, auc_err_raw, nzcoeffs_median_raw = tune_l1_ratio(X, k, alpha, l1_ratios)\n",
    "    accuracy_avg_onehot, accuracy_err_onehot, auc_avg_onehot, auc_err_onehot, nzcoeffs_median_onehot = tune_l1_ratio(X_one_hot, k, alpha, l1_ratios)\n",
    "\n",
    "    # Accuracy\n",
    "    axes[0, 0].plot(nzcoeffs_median_raw, accuracy_avg_raw, label='alpha %.0e' % alpha)\n",
    "    #axes[0, 0].fill_between(nzcoeffs_median_raw, [x+e for x, e in zip(accuracy_avg_raw, accuracy_err_raw)], [x-e for x, e in zip(accuracy_avg_raw, accuracy_err_raw)], alpha=0.5)\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_ylim([0.6, 1])\n",
    "    axes[0, 0].set_title('Raw data Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "\n",
    "    axes[0, 1].plot(nzcoeffs_median_onehot, accuracy_avg_onehot, label='alpha %.0e' % alpha)\n",
    "    #axes[0, 1].fill_between(nzcoeffs_median_onehot, [x+e for x, e in zip(accuracy_avg_onehot, accuracy_err_onehot)], [x-e for x, e in zip(accuracy_avg_onehot, accuracy_err_onehot)], alpha=0.5)\n",
    "    axes[0, 1].set_ylim([0.6, 1])\n",
    "    axes[0, 1].set_title('One-hot data Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # AUC\n",
    "    axes[1, 0].plot(nzcoeffs_median_raw, auc_avg_raw, label='alpha %.0e' % alpha)\n",
    "    #axes[1, 0].fill_between(nzcoeffs_median_raw, [x+e for x, e in zip(auc_avg_raw, auc_err_raw)], [x-e for x, e in zip(auc_avg_raw, auc_err_raw)], alpha=0.5)\n",
    "    axes[1, 0].set_xlabel('Features')\n",
    "    axes[1, 0].set_ylabel('AUC')\n",
    "    axes[1, 0].set_ylim([0.6, 1])\n",
    "    axes[1, 0].set_title('Raw data AUC')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    axes[1, 1].plot(nzcoeffs_median_onehot, auc_avg_onehot, label='alpha %.0e' % alpha)\n",
    "    #axes[1, 1].fill_between(nzcoeffs_median_onehot, [x+e for x, e in zip(auc_avg_onehot, auc_err_onehot)], [x-e for x, e in zip(auc_avg_onehot, auc_err_onehot)], alpha=0.5)\n",
    "    axes[1, 1].set_xlabel('Features')\n",
    "    axes[1, 1].set_ylim([0.6, 1])\n",
    "    axes[1, 1].set_title('One-hot AUC')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these plots, we can easily choose alpha for each dataset. For the raw data, we should use alpha=0.1. For the one-hot data, we should use alpha=0.01, unless we want a model with less than 20 or so features, in which case we should use alpha=0.1, but we need to be careful because accuracy and AUC drop off quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better regularization for one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxpy import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "m, n = X_one_hot.shape\n",
    "\n",
    "fused = np.zeros((n+1, n))\n",
    "# define fused lasso matrix\n",
    "index = 0\n",
    "for i in range(30):\n",
    "    start, end = q_to_start_index[i], q_to_start_index[i+1]\n",
    "    fused[index, start] = 1\n",
    "    index += 1\n",
    "    for j in range(start, end-1):\n",
    "        fused[index, j] = -1\n",
    "        fused[index, j+1] = 1\n",
    "        index += 1\n",
    "\n",
    "# tune fused regularization\n",
    "accuracy_avg = []\n",
    "accuracy_err = []\n",
    "auc_avg = []\n",
    "auc_err = []\n",
    "nzcoeffs_median = []\n",
    "\n",
    "lambdas = np.logspace(-2, 1.5, 20)\n",
    "for lamb in lambdas:\n",
    "    accuracy = np.zeros((k,))\n",
    "    auroc = np.zeros((k,))\n",
    "    nzcoeffs = np.zeros((k,))\n",
    "\n",
    "    for i, (split_test_indices, split_test_videos) in enumerate(zip(*split_by_video(train_videos, k))):\n",
    "        split_train_indices = list(set(train_indices) - set(split_test_indices))\n",
    "\n",
    "        # fit model\n",
    "        beta = Variable(n)\n",
    "        beta0 = Variable()\n",
    "        objective = Minimize(sum_entries(logistic(mul_elemwise(-((2*y)-1)[split_train_indices], (X_one_hot[split_train_indices, :] * beta)+beta0))) + \\\n",
    "                             0.1*norm(beta, 2) + lamb*norm(fused * beta, 1))\n",
    "        prob = Problem(objective, [])\n",
    "        result = prob.solve()\n",
    "\n",
    "        b = np.array(beta.value).reshape((n,))\n",
    "        b[np.abs(b) < pow(10, -7)] = 0\n",
    "        yhat = beta0.value+(X_one_hot[split_test_indices, :].dot(b))\n",
    "        fpr, tpr, thresholds = roc_curve(y[split_test_indices], yhat)\n",
    "        nnz = np.sum(~np.isclose(b[1:], b[0:-1]) & (b[0:-1]!=0))\n",
    "\n",
    "        # accuracy\n",
    "        accuracy[i] = accuracy_score(y[split_test_indices], yhat>0)\n",
    "\n",
    "        # auc\n",
    "        auroc[i] = auc(fpr, tpr)\n",
    "            \n",
    "        # nonzero coeffs\n",
    "        nzcoeffs[i] = nnz\n",
    "\n",
    "    # calculate mean and 95% confidence intervals\n",
    "    accuracy_avg.append(np.mean(accuracy))\n",
    "    ci = st.t.interval(0.95, k-1, loc=np.mean(accuracy), scale=st.sem(accuracy))\n",
    "    accuracy_err.append((ci[1]-ci[0])/2)\n",
    "    auc_avg.append(np.mean(auroc))\n",
    "    ci = st.t.interval(0.95, k-1, loc=np.mean(auroc), scale=st.sem(auroc))\n",
    "    auc_err.append((ci[1]-ci[0])/2)\n",
    "    nzcoeffs_median.append(np.median(nzcoeffs))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "# Accuracy\n",
    "plt.plot(nzcoeffs_median, accuracy_avg)\n",
    "plt.fill_between(nzcoeffs_median, [x+e for x, e in zip(accuracy_avg, accuracy_err)], [x-e for x, e in zip(accuracy_avg, accuracy_err)], alpha=0.5, label='accuracy')\n",
    "\n",
    "# AUC\n",
    "plt.plot(nzcoeffs_median, auc_avg)\n",
    "plt.fill_between(nzcoeffs_median, [x+e for x, e in zip(auc_avg, auc_err)], [x-e for x, e in zip(auc_avg, auc_err)], alpha=0.5, label='auc')\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best L2\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l2', alpha=0.01, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n",
    "\n",
    "yhat = sgdc.predict_proba(X[test_indices, :])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "plt.plot(fpr, tpr, label='L2 (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best Raw data L1\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='elasticnet', alpha=0.1, l1_ratio=0.9, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n",
    "\n",
    "yhat = sgdc.predict_proba(X[test_indices, :])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "print('questions used', [header[i] for i, x in enumerate(sgdc.coef_[0]) if x != 0])\n",
    "plt.plot(fpr, tpr, label='L1 raw data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best On-hot data L1\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='elasticnet', alpha=0.01, l1_ratio=0.9, fit_intercept=True)\n",
    "sgdc.fit(X_one_hot[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X_one_hot[train_indices, :], y[train_indices], X_one_hot[test_indices, :], y[test_indices])\n",
    "\n",
    "yhat = sgdc.predict_proba(X_one_hot[test_indices, :])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "print('questions used', [header_one_hot[i] for i, x in enumerate(sgdc.coef_[0]) if x != 0])\n",
    "plt.plot(fpr, tpr, label='L1 one-hot data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best On-hot fused\n",
    "beta = Variable(n)\n",
    "beta0 = Variable()\n",
    "objective = Minimize(sum_entries(logistic(mul_elemwise(-((2*y)-1)[train_indices], (X_one_hot[train_indices, :] * beta)+beta0))) + \\\n",
    "                             0.1*norm(beta, 2) + 0*norm(beta, 1) + 10*norm(fused * beta, 1))\n",
    "prob = Problem(objective, [])\n",
    "result = prob.solve()\n",
    "\n",
    "b = np.array(beta.value).reshape((n,))\n",
    "b[np.abs(b) < pow(10, -7)] = 0\n",
    "nnz = np.sum(~np.isclose(b[1:], b[0:-1]) & (b[0:-1]!=0))\n",
    "print('nonzero coefficients:', nnz, 'out of', X_one_hot.shape[1])\n",
    "yhat = beta0.value+(X_one_hot[train_indices, :].dot(b))\n",
    "fpr, tpr, thresholds = roc_curve(y[train_indices], yhat)\n",
    "print('train accuracy:', accuracy_score(y[train_indices], yhat>0))\n",
    "print('train auc:', auc(fpr, tpr))\n",
    "yhat = beta0.value+(X_one_hot[test_indices, :].dot(b))\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "print('test accuracy:', accuracy_score(y[test_indices], yhat>0))\n",
    "print('test auc:', auc(fpr, tpr))\n",
    "print('questions used', [header_one_hot[i] for i, x in enumerate(b) if b[i] != 0 and not np.isclose(b[i], b[i-1])])\n",
    "plt.plot(fpr, tpr, label='Fused one-hot data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our L1-regularized models are performing about as well as our L2 model, even though they're using fewer features. Also, even though the L1 one-hot data model is using more features, those features are much simpler (yes/no questions) than the features used in the L2 and L1 raw data model where each question has 6 responses. I chose parameters for these three models by looking at the cross-validation plots. Since the error bars on our cross-validation runs are fairly large, we really don't need to tune more precisely than this. Next, let's try to really crank up the regularization to produce L1 models with as few features as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best L2\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l2', alpha=0.01, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n",
    "\n",
    "yhat = sgdc.predict_proba(X[test_indices, :])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "plt.plot(fpr, tpr, label='L2 (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best Raw data L1\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='elasticnet', alpha=0.1, l1_ratio=0.95, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n",
    "\n",
    "yhat = sgdc.predict_proba(X[test_indices, :])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "print('questions used', [header[i] for i, x in enumerate(sgdc.coef_[0]) if x != 0])\n",
    "plt.plot(fpr, tpr, label='L1 raw data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best One-hot data L1\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='elasticnet', alpha=0.1, l1_ratio=0.9, fit_intercept=True)\n",
    "sgdc.fit(X_one_hot[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X_one_hot[train_indices, :], y[train_indices], X_one_hot[test_indices, :], y[test_indices])\n",
    "\n",
    "yhat = sgdc.predict_proba(X_one_hot[test_indices, :])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "print('questions used', [header_one_hot[i] for i, x in enumerate(sgdc.coef_[0]) if x != 0])\n",
    "plt.plot(fpr, tpr, label='L1 one-hot data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best On-hot fused\n",
    "beta = Variable(n)\n",
    "beta0 = Variable()\n",
    "objective = Minimize(sum_entries(logistic(mul_elemwise(-((2*y[train_indices])-1), (X_one_hot[train_indices, :] * beta)+beta0))) + \\\n",
    "                             0.1*norm(beta, 2) + 0*norm(beta, 1) + 30*norm(fused * beta, 1))\n",
    "prob = Problem(objective, [])\n",
    "result = prob.solve()\n",
    "\n",
    "b = np.array(beta.value).reshape((n,))\n",
    "b[np.abs(b) < pow(10, -7)] = 0\n",
    "nnz = np.sum(~np.isclose(b[1:], b[0:-1]) & (b[0:-1]!=0))\n",
    "print('nonzero coefficients:', nnz, 'out of', X_one_hot.shape[1])\n",
    "yhat = beta0.value+(X_one_hot[train_indices, :].dot(b))\n",
    "fpr, tpr, thresholds = roc_curve(y[train_indices], yhat)\n",
    "print('train accuracy:', accuracy_score(y[train_indices], yhat>0))\n",
    "print('train auc:', auc(fpr, tpr))\n",
    "yhat = beta0.value+(X_one_hot[test_indices, :].dot(b))\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "print('test accuracy:', accuracy_score(y[test_indices], yhat>0))\n",
    "print('test auc:', auc(fpr, tpr))\n",
    "print('questions used', [header_one_hot[i] for i, x in enumerate(b) if b[i] != 0 and not np.isclose(b[i], b[i-1])])\n",
    "plt.plot(fpr, tpr, label='Fused one-hot data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a small number of features here and the AUC's look pretty good, but the accuracies are low. This ties back to the very first plot - why is it that increased regularization corresponds to better AUC's but lower accuracy?\n",
    "\n",
    "We can think of a classifier like this as a method for predicting whether or not the child in the video has autism. But we can also think of it as a method for ordering all of the children from most likely to have autism to least likely to have autism. In order to make good predictions, we need to correctly order the children and then correctly choose a cutoff so that we can say all the children with a score above the cutoff have autism and all of the others don't. We might get the ordering right, but if we get the cutoff wrong, our accuracy will be bad. An AUC curve tells you if your ordering is good. Accuracy tells you if your ordering AND your cutoff are good. \n",
    "\n",
    "A linear model like this will set the cutoff based on the prevalence of autism in the training data. Since we have a small dataset, the prevalence of autism in the training set might be slightly different than the prevalence of autism in the test set, just by chance. This will cause the model to choose a bad cutoff, but the ordering it produces might still be good. I think that's what's happening here. The problem will get worse if you try to run this model on a new dataset with a very different prevalence.\n",
    "\n",
    "However, I think you can make the case that since the purpose of your method is to triage children so that clinicians can diagnose them more quickly, then maybe you don't need to worry about the cutoff. It may be that being able to order children from most likely to have autism to least likely is sufficient. In this case, you don't need to worry too much about accuracy and you can focus on finding a model with a good AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prevalence = np.sum(y[train_indices])\n",
    "print('train prevalence', np.sum(y[train_indices]/len(train_indices)))\n",
    "print('test prevalence', np.sum(y[test_indices]/len(test_indices)))\n",
    "print(np.shape(train_indices), np.shape(test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at train prevalence vs test prevalance, we see they they do differ by quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best L2\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l2', alpha=0.01, fit_intercept=True)\n",
    "sgdc.fit(X, y)\n",
    "print_stats(sgdc, X, y, Xind, yind)\n",
    "\n",
    "yhat = sgdc.predict_proba(Xind)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(yind, yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "l2_beta = sgdc.coef_[0]\n",
    "plt.plot(fpr, tpr, label='L2 (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best Raw data L1\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='elasticnet', alpha=0.1, l1_ratio=0.9, fit_intercept=True)\n",
    "sgdc.fit(X, y)\n",
    "print_stats(sgdc, X, y, Xind, yind)\n",
    "\n",
    "yhat = sgdc.predict_proba(Xind)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(yind, yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "l1_beta = sgdc.coef_[0]\n",
    "print('questions used', [header[i] for i, x in enumerate(sgdc.coef_[0]) if x != 0])\n",
    "plt.plot(fpr, tpr, label='L1 raw data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best On-hot data L1\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='elasticnet', alpha=0.01, l1_ratio=0.9, fit_intercept=True)\n",
    "sgdc.fit(X_one_hot, y)\n",
    "print_stats(sgdc, X_one_hot, y, Xind_one_hot, yind)\n",
    "\n",
    "yhat = sgdc.predict_proba(Xind_one_hot)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(yind, yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "l1_onehot_beta = sgdc.coef_[0]\n",
    "print('questions used', [header_one_hot[i] for i, x in enumerate(sgdc.coef_[0]) if x != 0])\n",
    "plt.plot(fpr, tpr, label='L1 one-hot data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best One-hot fused\n",
    "beta = Variable(X_one_hot.shape[1])\n",
    "beta0 = Variable()\n",
    "objective = Minimize(sum_entries(logistic(mul_elemwise(-((2*y)-1), (X_one_hot * beta)+beta0))) + \\\n",
    "                             0.1*norm(beta, 2) + 10*norm(fused * beta, 1))\n",
    "prob = Problem(objective, [])\n",
    "result = prob.solve()\n",
    "\n",
    "b = np.array(beta.value).reshape((X_one_hot.shape[1],))\n",
    "b[np.abs(b) < pow(10, -7)] = 0\n",
    "nnz = np.sum(~np.isclose(b[1:], b[0:-1]) & (b[0:-1]!=0))\n",
    "l1_fused_beta = b\n",
    "print('nonzero coefficients:', nnz, 'out of', X_one_hot.shape[1])\n",
    "yhat = beta0.value+(X_one_hot.dot(b))\n",
    "fpr, tpr, thresholds = roc_curve(y, yhat)\n",
    "print('train accuracy:', accuracy_score(y, yhat>0))\n",
    "print('train auc:', auc(fpr, tpr))\n",
    "yhat = beta0.value+(Xind_one_hot.dot(b))\n",
    "fpr, tpr, thresholds = roc_curve(yind, yhat)\n",
    "print('test accuracy:', accuracy_score(yind, yhat>0))\n",
    "print('test auc:', auc(fpr, tpr))\n",
    "print('questions used', [header_one_hot[i] for i, x in enumerate(b) if b[i] != 0 and not np.isclose(b[i], b[i-1])])\n",
    "plt.plot(fpr, tpr, label='Fused one-hot data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l1_onehot_beta)\n",
    "print(l1_fused_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature functions\n",
    "plt.figure(figsize=(20, 20))\n",
    "for q in range(30):\n",
    "    plt.subplot(6, 5, q+1)\n",
    "    responses = [x[1] for x in header_one_hot if x[0] == 'question%d' % (q+1) and x[1] < 7]\n",
    "    plt.plot(responses, [l2_beta[q]*r for r in responses], label='l2')\n",
    "    plt.plot(responses, [l1_beta[q]*r for r in responses], label='l1')\n",
    "    plt.plot(responses, [l1_onehot_beta[i] for i in range(q_to_start_index[q], q_to_start_index[q]+len(responses))], label='l1_onehot')\n",
    "    plt.plot(responses, [l1_fused_beta[i] for i in range(q_to_start_index[q], q_to_start_index[q]+len(responses))], label='fused_onehot')\n",
    "    plt.ylim([-1, 1])\n",
    "    plt.title('Question %d' % (q+1))\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Look at concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_scorers = sorted(set([x[0] for x in metadata]))\n",
    "unique_videos = sorted(set([x[1] for x in metadata]))\n",
    "scorer_to_index = dict([(x, i) for i, x in enumerate(unique_scorers)])\n",
    "video_to_index = dict([(x, i) for i, x in enumerate(unique_videos)])\n",
    "\n",
    "X_conc = np.zeros((len(unique_scorers), len(unique_videos), X.shape[1]))-1\n",
    "for i, m in enumerate(metadata):\n",
    "    X_conc[scorer_to_index[m[0]], video_to_index[m[1]], :] = X[i, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance between raters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "q = 10\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(2, 3, 1)\n",
    "cm = confusion_matrix(X_conc[0, :, q], X_conc[1, :, q])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('0 vs 1')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "cm = confusion_matrix(X_conc[1, :, q], X_conc[2, :, q])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('1 vs 2')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "cm = confusion_matrix(X_conc[2, :, q], X_conc[1, :, q])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('2 vs 1')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "cm = confusion_matrix(X_conc[0, :, q], X_conc[2, :, q])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('0 vs 2')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "cm = confusion_matrix(X_conc[1, :, q], X_conc[0, :, q])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('1 vs 0')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "cm = confusion_matrix(X_conc[2, :, q], X_conc[0, :, q])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('2 vs 0')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance between videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "v = 10\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(2, 3, 1)\n",
    "cm = confusion_matrix(X_conc[0, v, :], X_conc[1, v, :])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('0 vs 1')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "cm = confusion_matrix(X_conc[1, v, :], X_conc[2, v, :])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('1 vs 2')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "cm = confusion_matrix(X_conc[2, v, :], X_conc[1, v, :])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('2 vs 1')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "cm = confusion_matrix(X_conc[0, v, :], X_conc[2, v, :])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('0 vs 2')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "cm = confusion_matrix(X_conc[1, v, :], X_conc[0, v, :])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('1 vs 0')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "cm = confusion_matrix(X_conc[2, v, :], X_conc[0, v, :])\n",
    "plt.imshow(cm/np.sum(cm, axis=0), vmin=0, vmax=1)\n",
    "plt.title('2 vs 0')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
