{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "metadata = []\n",
    "\n",
    "with open('primary_dataset.csv', 'r') as f:\n",
    "    data = list(csv.reader(f))\n",
    "\n",
    "header = data[0][6:36]\n",
    "for line in data[1:]:\n",
    "    scorer, video, diagnosis, age, sex = line[2], line[3], line[36], line[37], line[38]\n",
    "    qs = line[6:36]\n",
    "        \n",
    "    # Remove scores with missing questions\n",
    "    if len([x for x in qs if x == '']) == 0:\n",
    "        X.append([int(x) for x in qs])\n",
    "        y.append(int(diagnosis == 'asd'))\n",
    "        metadata.append([scorer, video, age, sex])\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "m, n = X.shape\n",
    "\n",
    "print('X', X.shape)\n",
    "print('y', y.shape)\n",
    "print('videos', len(set([x[1] for x in metadata])))\n",
    "print('scorers', len(set([x[0] for x in metadata])))\n",
    "print('% missing', np.sum(X>=7)/(X.shape[0]*X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test\n",
    "We tried splitting by video and also by rater. At first, we thought correlation might exists between two different raters rating the same video. However, after some experimentation we noticed that each rater seems to have his/her own technique and we were overfitting to rater more than to video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "def split_by_video(videos, k):\n",
    "    shuffled_videos = list(videos)\n",
    "    random.shuffle(shuffled_videos)\n",
    "    num_videos_per_group = int(math.floor(len(shuffled_videos)/k))\n",
    "    split = []\n",
    "    videos = []\n",
    "    for i in range(k):\n",
    "        split_videos = set(shuffled_videos[(num_videos_per_group*i):(num_videos_per_group*(i+1))])\n",
    "        split.append(list(np.where([x[1] in split_videos for x in metadata])[0]))\n",
    "        videos.append(split_videos)\n",
    "    return split, videos\n",
    "\n",
    "def split_by_rater(raters, k):\n",
    "    shuffled_raters = list(raters)\n",
    "    random.shuffle(shuffled_raters)\n",
    "    num_raters_per_group = int(math.floor(len(shuffled_raters)/k))\n",
    "    split = []\n",
    "    raters = []\n",
    "    for i in range(k):\n",
    "        split_raters = set(shuffled_raters[(num_raters_per_group*i):(num_raters_per_group*(i+1))])\n",
    "        split.append(list(np.where([x[0] in split_raters for x in metadata])[0]))\n",
    "        raters.append(split_raters)\n",
    "    return split, raters\n",
    "\n",
    "\n",
    "unique_videos = set([x[1] for x in metadata])\n",
    "print('unique videos', len(unique_videos))\n",
    "\n",
    "unique_raters = set([x[0] for x in metadata])\n",
    "print('unique raters', len(unique_raters))\n",
    "\n",
    "split, videos = split_by_video(unique_videos, 5)\n",
    "test_indices, test_videos = np.asarray(split[0]), videos[0]\n",
    "train_indices, train_videos = np.asarray(list(chain.from_iterable(split[1:5]))), set(chain.from_iterable(videos[1:5]))\n",
    "\n",
    "#split, raters = split_by_rater(unique_raters, 3)\n",
    "#test_indices, test_raters = np.asarray(split[0]), raters[0]\n",
    "#train_indices, train_raters = np.asarray(list(chain.from_iterable(split[1:3]))), set(chain.from_iterable(raters[1:3]))\n",
    "\n",
    "print('train_videos', len(train_videos))\n",
    "print('test_videos', len(test_videos))\n",
    "#print('train raters', len(train_raters))\n",
    "#print('test raters', len(test_raters))\n",
    "print(train_indices.shape, test_indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic L1 and L2 Models\n",
    "Take a quick look at performance of L2 vs L1 regularization. However, we need to take these results with a grain of salt since I haven't tuned any of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def print_stats(model, X_train, y_train, X_test, y_test):\n",
    "    # Nonzero entries\n",
    "    print('nonzero coefficients:', np.sum(model.coef_[0]!=0), 'out of', model.coef_.shape[1])\n",
    "\n",
    "    # Accuracy\n",
    "    print('train accuracy:', sgdc.score(X_train, y_train))\n",
    "    print('test accuracy:', sgdc.score(X_test, y_test))\n",
    "    \n",
    "    # AUC\n",
    "    yhat = sgdc.predict_proba(X_train)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, yhat)\n",
    "    print('train auc:', auc(fpr, tpr))\n",
    "    yhat = sgdc.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, yhat)\n",
    "    print('test auc:', auc(fpr, tpr))\n",
    "\n",
    "# ---------------------------- L2 ----------------------------\n",
    "print('L2 Raw Data')\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l2', alpha=0.1, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n",
    "\n",
    "# ---------------------------- L1 ----------------------------\n",
    "print('\\nL1 Raw Data')\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l1', alpha=0.05, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune an L2 Model\n",
    "We start by tuning an L2 model. This let's us know the best possible performance we can expect from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "def tune_L2(X, k, alphas):\n",
    "    accuracy_avg = []\n",
    "    accuracy_err = []\n",
    "    auc_avg = []\n",
    "    auc_err = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        accuracy = np.zeros((k,))\n",
    "        auroc = np.zeros((k,))\n",
    "\n",
    "        for i, (split_test_indices, split_test_videos) in enumerate(zip(*split_by_video(train_videos, k))):\n",
    "            split_train_indices = list(set(train_indices) - set(split_test_indices))\n",
    "\n",
    "            # fit model\n",
    "            sgdc = SGDClassifier(loss=\"log\", penalty='l2', alpha=alpha, fit_intercept=True)\n",
    "            sgdc.fit(X[split_train_indices, :], y[split_train_indices])\n",
    "\n",
    "            # accuracy\n",
    "            accuracy[i] = sgdc.score(X[split_test_indices, :], y[split_test_indices])\n",
    "\n",
    "            # auc\n",
    "            yhat = sgdc.predict_proba(X[split_test_indices, :])[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y[split_test_indices], yhat)\n",
    "            auroc[i] = auc(fpr, tpr)\n",
    "\n",
    "        # calculate mean and 95% confidence intervals\n",
    "        accuracy_avg.append(np.mean(accuracy))\n",
    "        ci = st.t.interval(0.95, k-1, loc=np.mean(accuracy), scale=st.sem(accuracy))\n",
    "        accuracy_err.append((ci[1]-ci[0])/2)\n",
    "        auc_avg.append(np.mean(auroc))\n",
    "        ci = st.t.interval(0.95, k-1, loc=np.mean(auroc), scale=st.sem(auroc))\n",
    "        auc_err.append((ci[1]-ci[0])/2)\n",
    "    return accuracy_avg, accuracy_err, auc_avg, auc_err\n",
    "    \n",
    "\n",
    "k = 5\n",
    "alphas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "accuracy_avg_raw, accuracy_err_raw, auc_avg_raw, auc_err_raw = tune_L2(X, k, alphas)\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Accuracy\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(alphas, accuracy_avg_raw, label='raw data')\n",
    "plt.fill_between(alphas, [x+e for x, e in zip(accuracy_avg_raw, accuracy_err_raw)], [x-e for x, e in zip(accuracy_avg_raw, accuracy_err_raw)], alpha=0.5)\n",
    "\n",
    "plt.xlabel('Regularization Parameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.6, 1])\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "# AUC\n",
    "plt.subplot(2, 1, 2, sharex=ax1)\n",
    "\n",
    "plt.plot(alphas, auc_avg_raw, label='raw data')\n",
    "plt.fill_between(alphas, [x+e for x, e in zip(auc_avg_raw, auc_err_raw)], [x-e for x, e in zip(auc_avg_raw, auc_err_raw)], alpha=0.5)\n",
    "\n",
    "plt.xlabel('Regularization Parameter')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim([0.6, 1])\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune an Elastic Net Model\n",
    "The challenge in tuning an elastic net model is that we have two parameters to consider:\n",
    "    1. alpha: the amount of regularization \n",
    "    2. l1_ratio: the ratio between L1 and L2 regularization\n",
    "We're interested in understanding how performance (measured by accuracy and AUC) vary with the number of parameters used. To do this, we'll start by fixing alpha, and then varying the l1_ratio to see a range of models with differing number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_l1_ratio(X, k, alpha, l1_ratios):\n",
    "    accuracy_avg = []\n",
    "    accuracy_err = []\n",
    "    auc_avg = []\n",
    "    auc_err = []\n",
    "    nzcoeffs_median = []\n",
    "\n",
    "    for l1_ratio in l1_ratios:\n",
    "        accuracy = np.zeros((k,))\n",
    "        auroc = np.zeros((k,))\n",
    "        nzcoeffs = np.zeros((k,))\n",
    "\n",
    "        for i, (split_test_indices, split_test_videos) in enumerate(zip(*split_by_video(train_videos, k))):\n",
    "            split_train_indices = list(set(train_indices) - set(split_test_indices))\n",
    "\n",
    "            # fit model\n",
    "            sgdc = SGDClassifier(loss=\"log\", penalty='elasticnet', alpha=alpha, l1_ratio=l1_ratio, fit_intercept=True)\n",
    "            sgdc.fit(X[split_train_indices, :], y[split_train_indices])\n",
    "\n",
    "            # accuracy\n",
    "            accuracy[i] = sgdc.score(X[split_test_indices, :], y[split_test_indices])\n",
    "\n",
    "            # auc\n",
    "            yhat = sgdc.predict_proba(X[split_test_indices, :])[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y[split_test_indices], yhat)\n",
    "            auroc[i] = auc(fpr, tpr)\n",
    "            \n",
    "            # nonzero coeffs\n",
    "            nzcoeffs[i] = np.sum(sgdc.coef_[0]!=0)\n",
    "\n",
    "        # calculate mean and 95% confidence intervals\n",
    "        accuracy_avg.append(np.mean(accuracy))\n",
    "        ci = st.t.interval(0.95, k-1, loc=np.mean(accuracy), scale=st.sem(accuracy))\n",
    "        accuracy_err.append((ci[1]-ci[0])/2)\n",
    "        auc_avg.append(np.mean(auroc))\n",
    "        ci = st.t.interval(0.95, k-1, loc=np.mean(auroc), scale=st.sem(auroc))\n",
    "        auc_err.append((ci[1]-ci[0])/2)\n",
    "        nzcoeffs_median.append(np.median(nzcoeffs))\n",
    "    return accuracy_avg, accuracy_err, auc_avg, auc_err, nzcoeffs_median\n",
    "\n",
    "k = 5\n",
    "alphas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "l1_ratios = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex='col', sharey=True, figsize=(10, 10))\n",
    "axes[0, 0].axhline(y=0.85, linestyle='--', color='black', alpha=0.5)\n",
    "axes[0, 1].axhline(y=0.85, linestyle='--', color='black', alpha=0.5)\n",
    "axes[1, 0].axhline(y=0.95, linestyle='--', color='black', alpha=0.5)\n",
    "axes[1, 1].axhline(y=0.95, linestyle='--', color='black', alpha=0.5)\n",
    "\n",
    "for alpha in alphas:\n",
    "    accuracy_avg_raw, accuracy_err_raw, auc_avg_raw, auc_err_raw, nzcoeffs_median_raw = tune_l1_ratio(X, k, alpha, l1_ratios)\n",
    "    # Accuracy\n",
    "    axes[0, 0].plot(nzcoeffs_median_raw, accuracy_avg_raw, label='alpha %.0e' % alpha)\n",
    "    #axes[0, 0].fill_between(nzcoeffs_median_raw, [x+e for x, e in zip(accuracy_avg_raw, accuracy_err_raw)], [x-e for x, e in zip(accuracy_avg_raw, accuracy_err_raw)], alpha=0.5)\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_ylim([0.6, 1])\n",
    "    axes[0, 0].set_title('Raw data Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    \n",
    "    # AUC\n",
    "    axes[1, 0].plot(nzcoeffs_median_raw, auc_avg_raw, label='alpha %.0e' % alpha)\n",
    "    #axes[1, 0].fill_between(nzcoeffs_median_raw, [x+e for x, e in zip(auc_avg_raw, auc_err_raw)], [x-e for x, e in zip(auc_avg_raw, auc_err_raw)], alpha=0.5)\n",
    "    axes[1, 0].set_xlabel('Features')\n",
    "    axes[1, 0].set_ylabel('AUC')\n",
    "    axes[1, 0].set_ylim([0.6, 1])\n",
    "    axes[1, 0].set_title('Raw data AUC')\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these plots, we can easily choose alpha for each dataset. For the raw data, we should use alpha=0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Best L2\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='l2', alpha=0.01, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n",
    "\n",
    "yhat = sgdc.predict_proba(X[test_indices, :])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "plt.plot(fpr, tpr, label='L2 (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "# Best Raw data L1\n",
    "sgdc = SGDClassifier(loss=\"log\", penalty='elasticnet', alpha=0.1, l1_ratio=0.9, fit_intercept=True)\n",
    "sgdc.fit(X[train_indices, :], y[train_indices])\n",
    "print_stats(sgdc, X[train_indices, :], y[train_indices], X[test_indices, :], y[test_indices])\n",
    "\n",
    "yhat = sgdc.predict_proba(X[test_indices, :])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y[test_indices], yhat)\n",
    "nnz = np.sum(sgdc.coef_[0]!=0)\n",
    "print('questions used', [header[i] for i, x in enumerate(sgdc.coef_[0]) if x != 0])\n",
    "plt.plot(fpr, tpr, label='L1 raw data (features=%d, auc=%f)' % (nnz, auc(fpr, tpr)))\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our L1-regularized models are performing about as well as our L2 model, even though they're using fewer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prevalence = np.sum(y[train_indices])\n",
    "print('train prevalence', np.sum(y[train_indices]/len(train_indices)))\n",
    "print('test prevalence', np.sum(y[test_indices]/len(test_indices)))\n",
    "print(np.shape(train_indices), np.shape(test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at train prevalence vs test prevalance, we see they they do differ by quite a bit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
